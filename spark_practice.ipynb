{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Because spark already run a default SparkContext when start\n",
    "# SparkContext.stop(SparkContext)\n",
    "sc = SparkContext(master='local[4]')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"Python Spark\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", 360000) \\\n",
    "    .config(\"spark.akka.timeout\", 1000000) \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "raw_df = spark.read.parquet(\"file:///Users/Daniel_Lin/Desktop/Spark Practice/sample_data.snappy.parquet\")\n",
    "    \n",
    "# raw_df = spark.read.parquet(\"file:///Users/Daniel_Lin/Desktop/Task1.3/SparkData/2017-07-03/*\",\n",
    "#                            \"file:///Users/Daniel_Lin/Desktop/Task1.3/SparkData/2017-07-04/*\",\n",
    "#                            \"file:///Users/Daniel_Lin/Desktop/Task1.3/SparkData/2017-07-05/*\",\n",
    "#                            \"file:///Users/Daniel_Lin/Desktop/Task1.3/SparkData/2017-07-06/*\",\n",
    "#                            \"file:///Users/Daniel_Lin/Desktop/Task1.3/SparkData/2017-07-07/*\",\n",
    "#                            \"file:///Users/Daniel_Lin/Desktop/Task1.3/SparkData/2017-07-08/*\",\n",
    "#                            \"file:///Users/Daniel_Lin/Desktop/Task1.3/SparkData/2017-07-09/*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "source": [
    "### Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+-------+\n",
      "| os|app_name|version|  count|\n",
      "+---+--------+-------+-------+\n",
      "|And|      BC| 2.10.0|      3|\n",
      "|And|      BC| 2.16.0|      3|\n",
      "|And|     YCN| 1.22.1|     87|\n",
      "|And|     YCP| 5.20.4|     44|\n",
      "|And|     YMK| 5.20.7|1514330|\n",
      "|iOS|     YCN| 1.12.0|      5|\n",
      "|iOS|     YCP| 5.20.6|      7|\n",
      "|iOS|     YMK| 5.20.0| 451302|\n",
      "+---+--------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "version_df = raw_df.select((raw_df.f_os).alias(\"os\"), (raw_df.f_app_name).alias(\"app_name\"), (raw_df.m_appversion).alias(\"version\")) \\\n",
    "              .groupby(\"os\", \"app_name\", \"version\") \\\n",
    "              .count() \\\n",
    "              .orderBy(\"os\", \"app_name\", col(\"count\").desc())\n",
    "            \n",
    "max_version_df = version_df.groupBy(version_df.os.alias(\"max_os\"), version_df.app_name.alias(\"max_app_name\")) \\\n",
    "                    .agg(max(\"count\").alias(\"max_value\"))\n",
    "\n",
    "top_version_df = version_df.join(broadcast(max_version_df), \\\n",
    "                            (version_df.os == max_version_df.max_os) & \\\n",
    "                            (version_df.app_name == max_version_df.max_app_name) & \\\n",
    "                            (version_df[\"count\"] == max_version_df.max_value)) \\\n",
    "                     .drop(\"max_os\", \"max_app_name\", \"max_value\")\n",
    "    \n",
    "top_version_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import time\n",
    "\n",
    "def findInvalidDays(date):    \n",
    "    try:\n",
    "        time.strptime(date, \"%Y-%m-%d\")\n",
    "        return 0\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "findInvalidDays = udf(findInvalidDays, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.496663533472409e-08"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "day_df = raw_df.select(raw_df.f_timestamp_day.alias(\"day\")) \\\n",
    "                    .rdd \\\n",
    "                    .map(lambda x: (x.day, )) \\\n",
    "                    .toDF([\"day\"])\n",
    "\n",
    "invalid_sum = day_df.withColumn(\"invalid_count\", findInvalidDays(day_df.day)).rdd.values().sum()\n",
    "\n",
    "invalid_day_ratio = invalid_sum / day_df.count()\n",
    "invalid_day_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "def findFeatureInMap(map):\n",
    "    for key in map:\n",
    "        pattern = re.search(\"(.*)_pattern\", key)\n",
    "        intensity = re.search(\"(.*)_intensity\", key)\n",
    "        \n",
    "        target = pattern if pattern else intensity\n",
    "        \n",
    "        if target:\n",
    "            return target.group(1)\n",
    "    \n",
    "\n",
    "findFeatureInMap = udf(findFeatureInMap, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|country|     feature|number|\n",
      "+-------+------------+------+\n",
      "|     A1|   haircolor|    37|\n",
      "|     AD|       looks|    16|\n",
      "|     AE|       looks|  1905|\n",
      "|     AF|       looks|   624|\n",
      "|     AG|       looks|   113|\n",
      "|     AL|    lipstick|  2201|\n",
      "|     AM|    lipstick|  1107|\n",
      "|     AO|    lipstick|   198|\n",
      "|     AP|       looks|    74|\n",
      "|     AR|    lipstick|  4934|\n",
      "|     AT|    lipstick|   634|\n",
      "|     AU|       looks|  1272|\n",
      "|     AW|       looks|    18|\n",
      "|     AW|face_contour|    18|\n",
      "|     AZ|    lipstick|  3177|\n",
      "|     BA|    lipstick|  1026|\n",
      "|     BB|    lipstick|    49|\n",
      "|     BD|    lipstick|  7553|\n",
      "|     BE|    lipstick|  1021|\n",
      "|     BF|       looks|   194|\n",
      "+-------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "raw_cf_df = raw_df.filter(raw_df.e_key == \"YMK_Tryout\") \\\n",
    "                .select(raw_df.f_country.alias(\"country\"), raw_df.e_segment_map.alias(\"map\")) \\\n",
    "        \n",
    "cf_df = raw_cf_df.withColumn(\"feature\", findFeatureInMap(raw_cf_df.map)).drop(\"map\")\n",
    "\n",
    "cf_count_df = cf_df.groupby(\"country\", \"feature\") \\\n",
    "                    .count() \\\n",
    "                    .orderBy(\"country\", col(\"count\").desc())\n",
    "cf_count_df = cf_count_df.withColumnRenamed(\"count\", \"number\")\n",
    "\n",
    "cf_max_df = cf_count_df.groupby(\"country\") \\\n",
    "                        .agg(max(\"number\").alias(\"max_number\")) \\\n",
    "                        .orderBy(\"country\")\n",
    "cf_max_df = cf_max_df.withColumnRenamed(\"country\", \"max_country\")\n",
    "\n",
    "\n",
    "cond = [cf_count_df.country == cf_max_df.max_country, cf_count_df.number == cf_max_df.max_number]\n",
    "\n",
    "cf_top_df = cf_count_df.join(broadcast(cf_max_df), cond) \\\n",
    "                        .drop(\"max_country\") \\\n",
    "                        .drop(\"max_number\")\n",
    "cf_top_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question4 version1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convertDateToWeekday(date):\n",
    "    try:\n",
    "        return datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%A\")\n",
    "    except:\n",
    "        return \"undefine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|  weekday| count|\n",
      "+---------+------+\n",
      "|   Monday|771759|\n",
      "|   Sunday|131635|\n",
      "| Saturday| 65991|\n",
      "|   Friday| 48836|\n",
      "| Thursday| 37332|\n",
      "|Wednesday| 32154|\n",
      "|  Tuesday| 30586|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "date_df = raw_df.filter(raw_df.e_key == \"YMK_Launcher_Banner\") \\\n",
    "                .filter(raw_df.e_segment_map.getItem(\"operation\") == \"show\") \\\n",
    "                .select(raw_df.e_timestamp_day.alias(\"date\"))\n",
    "        \n",
    "weekday_df = date_df.rdd \\\n",
    "                    .map(lambda row: (convertDateToWeekday(row.date), )) \\\n",
    "                    .toDF([\"weekday\"])\n",
    "        \n",
    "weekday_count_df = weekday_df.groupBy(\"weekday\") \\\n",
    "                            .count() \\\n",
    "                            .orderBy(col(\"count\").desc())\n",
    "\n",
    "weekday_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question4 version2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def convertDateToWeekday(date):\n",
    "    try:\n",
    "        return datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%A\")\n",
    "    except:\n",
    "        return \"undefine\"\n",
    "    \n",
    "convertDateToWeekday = udf(convertDateToWeekday, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|  weekday| count|\n",
      "+---------+------+\n",
      "|   Monday|771759|\n",
      "|   Sunday|131635|\n",
      "| Saturday| 65991|\n",
      "|   Friday| 48836|\n",
      "| Thursday| 37332|\n",
      "|Wednesday| 32154|\n",
      "|  Tuesday| 30586|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "date_df = raw_df.filter(raw_df.e_key == \"YMK_Launcher_Banner\") \\\n",
    "                .filter(raw_df.e_segment_map.getItem(\"operation\") == \"show\") \\\n",
    "                .select(raw_df.e_timestamp_day.alias(\"date\"))\n",
    "        \n",
    "weekday_df = date_df.withColumn(\"weekday\", convertDateToWeekday(date_df.date)) \\\n",
    "                    .drop(\"date\")\n",
    "        \n",
    "weekday_count_df = weekday_df.groupBy(\"weekday\") \\\n",
    "                            .count() \\\n",
    "                            .orderBy(col(\"count\").desc())\n",
    "\n",
    "weekday_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://docs.google.com/document/d/1SRpfGNc6rN0qYgkgjtDxWnVgvWfuwMOqb5S5cznngmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
